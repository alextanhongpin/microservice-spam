{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import concurrent.futures\n",
    "# from timeit import default_timer as timer\n",
    "\n",
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# NOTE: classification_report contains accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# print(joblib.cpu_count())\n",
    "# print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load our sample data for both spam and ham. For this, we created a utility called loader which, given an input file path, reads the content from the file and append it to a python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loader(file_input):\n",
    "    data = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(file_input):\n",
    "        for file in filenames:\n",
    "            path = os.path.join(dirpath, file)\n",
    "            with open(path, encoding='latin-1') as f:\n",
    "                data.append(f.read())\n",
    "                f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_input = './data/enron1/ham'\n",
    "ham = loader(file_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_input = './data/enron1/spam'\n",
    "spam = loader(file_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_data = [(words, 0) for words in ham] # 0 for ham\n",
    "spam_data = [(words, 1) for words in spam] # 1 for spam\n",
    "all_data = spam_data + ham_data\n",
    "'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = np.array(all_data)\n",
    "X = all_data[:, 0]\n",
    "y = all_data[:, 1]\n",
    "'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mapper(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip = True, stem = True, symbols = True, stemmer = None):\n",
    "        self.strip = strip\n",
    "        self.stem = stem\n",
    "        self.symbols = symbols\n",
    "        self.stemmer = stemmer or PorterStemmer()\n",
    "        self.pattern = re.compile(r'\\W')\n",
    "        \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # ProcessPoolExecutor is for CPU intensive stuff.\n",
    "        # ThreadPoolExecutor is better suited for network operations or I/O.\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers = os.cpu_counts()) as executor:\n",
    "            pids = [executor.submit(self.tokenize, sentence) for sentence in X]\n",
    "            X_done = [pid.result() for pid in pids]\n",
    "            return X_done\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        words = word_tokenize(sentence)\n",
    "        out = []\n",
    "        for word in words:\n",
    "            if self.symbols and self.pattern.search(word):\n",
    "                continue\n",
    "            word = word.strip() if self.strip else word\n",
    "            word = self.stemmer.stem(word) if self.stem else word\n",
    "            out.append(word)\n",
    "        return ' '.join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_svc_clf():\n",
    "    from sklearn.svm import LinearSVC\n",
    "    \n",
    "    estimators = [('linear_svc', LinearSVC())]\n",
    "    param_grid = dict(linear_svc__C = [1, 10, 100],\n",
    "                      linear_svc__random_state = [42],\n",
    "                      linear_svc__max_iter = [1000, 10000])\n",
    "\n",
    "    return estimators, param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multinomial_nb_clf():\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "    estimators = [('multinomial_nb', MultinomialNB())]\n",
    "    param_grid = dict(multinomial_nb__alpha = [1, 10, 100],\n",
    "                      multinomial_nb__fit_prior = [True],\n",
    "                      multinomial_nb__class_prior = [None])\n",
    "\n",
    "    return estimators, param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_forest_clf():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    estimators = [('random_forest', RandomForestClassifier())]\n",
    "    param_grid = dict(random_forest__n_estimators = [10, 20, 30],\n",
    "                      random_forest__criterion = ['gini', 'entropy'],\n",
    "                      random_forest__max_features = ['auto', 'sqrt', 'log2'],\n",
    "#                       random_forest__max_depth = [None],\n",
    "                      random_forest__min_samples_split = [2],\n",
    "                      random_forest__min_samples_leaf = [1],\n",
    "                      random_forest__min_weight_fraction_leaf = [0],\n",
    "                      random_forest__max_leaf_nodes = [None],\n",
    "                      random_forest__min_impurity_decrease = [0],\n",
    "                      random_forest__bootstrap = [True],\n",
    "                      random_forest__oob_score = [False],\n",
    "                      random_forest__n_jobs = [-1],\n",
    "                      random_forest__random_state = [42],\n",
    "                      random_forest__warm_start = [False],\n",
    "                      random_forest__class_weight = ['balanced'])\n",
    "\n",
    "    return estimators, param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_nb_clf():\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    \n",
    "    estimators = [('gaussian_nb', GaussianNB())]\n",
    "    param_grid = dict()\n",
    "    return estimators, param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Classifier name: random_forest\n",
      "Best params:\n",
      " {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'auto', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 20, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False} \n",
      "\n",
      "Classifier results:\n",
      " {'mean_fit_time': array([  14.98731804,   14.53175004,   13.63925799,   12.15075421,\n",
      "         12.70283731,   13.35229111,   11.81410162,   11.9166801 ,\n",
      "         12.22733434,  102.97441045,   14.40297143,   14.48356374,\n",
      "         13.2359523 ,   13.28904843,   14.42465528,   12.65140525,\n",
      "         13.09362133,   14.4357899 ]), 'std_fit_time': array([  6.20726309e-01,   2.52685225e-01,   1.33368806e-01,\n",
      "         1.91034169e-01,   3.10355135e-01,   8.17819659e-02,\n",
      "         1.73340645e-01,   1.96042877e-01,   2.00577142e-01,\n",
      "         1.27295907e+02,   6.55420090e-01,   1.46325483e-01,\n",
      "         4.87393777e-01,   5.39816534e-01,   1.69141771e-01,\n",
      "         3.87083966e-01,   4.47012751e-01,   2.32048276e+00]), 'mean_score_time': array([ 8.96558698,  6.43733104,  5.16735458,  5.12062041,  5.12285058,\n",
      "        5.12623135,  5.24744757,  5.15685685,  5.27835345,  6.59136621,\n",
      "        6.27129833,  5.41200972,  5.70899367,  5.64254697,  5.63412301,\n",
      "        5.54288308,  6.13621934,  5.63792443]), 'std_score_time': array([ 2.2435655 ,  1.10105885,  0.08130857,  0.11874806,  0.08463255,\n",
      "        0.09483157,  0.05431769,  0.09959243,  0.30843331,  1.42045895,\n",
      "        0.14313615,  0.12125205,  0.23335266,  0.19734097,  0.37197679,\n",
      "        0.33342942,  0.37434166,  0.36878744]), 'param_random_forest__bootstrap': masked_array(data = [True True True True True True True True True True True True True True True\n",
      " True True True],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__class_weight': masked_array(data = ['balanced' 'balanced' 'balanced' 'balanced' 'balanced' 'balanced'\n",
      " 'balanced' 'balanced' 'balanced' 'balanced' 'balanced' 'balanced'\n",
      " 'balanced' 'balanced' 'balanced' 'balanced' 'balanced' 'balanced'],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__criterion': masked_array(data = ['gini' 'gini' 'gini' 'gini' 'gini' 'gini' 'gini' 'gini' 'gini' 'entropy'\n",
      " 'entropy' 'entropy' 'entropy' 'entropy' 'entropy' 'entropy' 'entropy'\n",
      " 'entropy'],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__max_features': masked_array(data = ['auto' 'auto' 'auto' 'sqrt' 'sqrt' 'sqrt' 'log2' 'log2' 'log2' 'auto'\n",
      " 'auto' 'auto' 'sqrt' 'sqrt' 'sqrt' 'log2' 'log2' 'log2'],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__max_leaf_nodes': masked_array(data = [None None None None None None None None None None None None None None None\n",
      " None None None],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__min_impurity_decrease': masked_array(data = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__min_samples_leaf': masked_array(data = [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__min_samples_split': masked_array(data = [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__min_weight_fraction_leaf': masked_array(data = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__n_estimators': masked_array(data = [10 20 30 10 20 30 10 20 30 10 20 30 10 20 30 10 20 30],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__n_jobs': masked_array(data = [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__oob_score': masked_array(data = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__random_state': masked_array(data = [42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42 42],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_random_forest__warm_start': masked_array(data = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'params': [{'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'auto', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 10, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'auto', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 20, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'auto', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 30, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'sqrt', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 10, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'sqrt', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 20, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'sqrt', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 30, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'log2', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 10, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'log2', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 20, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'gini', 'random_forest__max_features': 'log2', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 30, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'auto', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 10, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'auto', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 20, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'auto', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 30, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'sqrt', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 10, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'sqrt', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 20, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'sqrt', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 30, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'log2', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 10, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'log2', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 20, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}, {'random_forest__bootstrap': True, 'random_forest__class_weight': 'balanced', 'random_forest__criterion': 'entropy', 'random_forest__max_features': 'log2', 'random_forest__max_leaf_nodes': None, 'random_forest__min_impurity_decrease': 0, 'random_forest__min_samples_leaf': 1, 'random_forest__min_samples_split': 2, 'random_forest__min_weight_fraction_leaf': 0, 'random_forest__n_estimators': 30, 'random_forest__n_jobs': -1, 'random_forest__oob_score': False, 'random_forest__random_state': 42, 'random_forest__warm_start': False}], 'split0_test_score': array([ 0.93593074,  0.95411255,  0.95584416,  0.93593074,  0.95411255,\n",
      "        0.95584416,  0.91774892,  0.93506494,  0.94285714,  0.94632035,\n",
      "        0.95411255,  0.95584416,  0.94632035,  0.95411255,  0.95584416,\n",
      "        0.93160173,  0.93766234,  0.94372294]), 'split1_test_score': array([ 0.94545455,  0.95497835,  0.96277056,  0.94545455,  0.95497835,\n",
      "        0.96277056,  0.91774892,  0.94285714,  0.94805195,  0.95064935,\n",
      "        0.96363636,  0.96623377,  0.95064935,  0.96363636,  0.96623377,\n",
      "        0.91601732,  0.93852814,  0.95064935]), 'split2_test_score': array([ 0.95411255,  0.96363636,  0.96709957,  0.95411255,  0.96363636,\n",
      "        0.96709957,  0.90909091,  0.93506494,  0.95757576,  0.96103896,\n",
      "        0.97142857,  0.96623377,  0.96103896,  0.97142857,  0.96623377,\n",
      "        0.92900433,  0.94632035,  0.95584416]), 'mean_test_score': array([ 0.94516595,  0.95757576,  0.96190476,  0.94516595,  0.95757576,\n",
      "        0.96190476,  0.91486291,  0.93766234,  0.94949495,  0.95266955,\n",
      "        0.96305916,  0.96277056,  0.95266955,  0.96305916,  0.96277056,\n",
      "        0.92554113,  0.94083694,  0.95007215]), 'std_test_score': array([ 0.0074255 ,  0.00430005,  0.00463561,  0.0074255 ,  0.00430005,\n",
      "        0.00463561,  0.00408142,  0.00367328,  0.00609487,  0.00617632,\n",
      "        0.00708101,  0.00489771,  0.00617632,  0.00708101,  0.00489771,\n",
      "        0.00681732,  0.00389343,  0.00496527]), 'rank_test_score': array([13,  7,  5, 13,  7,  5, 18, 16, 12,  9,  1,  3,  9,  1,  3, 17, 15,\n",
      "       11], dtype=int32), 'split0_train_score': array([ 0.9987013,  0.9995671,  1.       ,  0.9987013,  0.9995671,\n",
      "        1.       ,  0.9978355,  1.       ,  1.       ,  0.9982684,\n",
      "        0.9991342,  1.       ,  0.9982684,  0.9991342,  1.       ,\n",
      "        0.9982684,  1.       ,  1.       ]), 'split1_train_score': array([ 0.9995671,  0.9995671,  1.       ,  0.9995671,  0.9995671,\n",
      "        1.       ,  0.9995671,  1.       ,  1.       ,  0.9982684,\n",
      "        0.9995671,  1.       ,  0.9982684,  0.9995671,  1.       ,\n",
      "        0.9982684,  1.       ,  1.       ]), 'split2_train_score': array([ 0.9987013,  1.       ,  1.       ,  0.9987013,  1.       ,\n",
      "        1.       ,  0.9987013,  1.       ,  1.       ,  0.9991342,\n",
      "        1.       ,  1.       ,  0.9991342,  1.       ,  1.       ,\n",
      "        0.9961039,  1.       ,  1.       ]), 'mean_train_score': array([ 0.9989899,  0.9997114,  1.       ,  0.9989899,  0.9997114,\n",
      "        1.       ,  0.9987013,  1.       ,  1.       ,  0.998557 ,\n",
      "        0.9995671,  1.       ,  0.998557 ,  0.9995671,  1.       ,\n",
      "        0.9975469,  1.       ,  1.       ]), 'std_train_score': array([ 0.00040814,  0.00020407,  0.        ,  0.00040814,  0.00020407,\n",
      "        0.        ,  0.00070692,  0.        ,  0.        ,  0.00040814,\n",
      "        0.00035346,  0.        ,  0.00040814,  0.00035346,  0.        ,\n",
      "        0.00102036,  0.        ,  0.        ])} \n",
      "\n",
      "Confusion matrix:\n",
      " [[ 477   39]\n",
      " [  26 1165]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.97      1191\n",
      "          1       0.95      0.92      0.94       516\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1707\n",
      "\n",
      "End\n",
      "Start\n",
      "Classifier name: gaussian_nb\n",
      "Best params:\n",
      " {} \n",
      "\n",
      "Classifier results:\n",
      " {'mean_fit_time': array([ 10.32190712]), 'std_fit_time': array([ 0.42621177]), 'mean_score_time': array([ 5.74431078]), 'std_score_time': array([ 0.04831143]), 'params': [{}], 'split0_test_score': array([ 0.94978355]), 'split1_test_score': array([ 0.94199134]), 'split2_test_score': array([ 0.94978355]), 'mean_test_score': array([ 0.94718615]), 'std_test_score': array([ 0.00367328]), 'rank_test_score': array([1], dtype=int32), 'split0_train_score': array([ 0.98787879]), 'split1_train_score': array([ 0.98787879]), 'split2_train_score': array([ 0.98787879]), 'mean_train_score': array([ 0.98787879]), 'std_train_score': array([ 0.])} \n",
      "\n",
      "Confusion matrix:\n",
      " [[ 476   40]\n",
      " [  55 1136]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.95      0.96      1191\n",
      "          1       0.90      0.92      0.91       516\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1707\n",
      "\n",
      "End\n",
      "model saved as random_forest.pkl\n",
      "model saved as gaussian_nb.pkl\n"
     ]
    }
   ],
   "source": [
    "classifiers = [# linear_svc_clf(),\n",
    "               # multinomial_nb_clf(),\n",
    "               random_forest_clf(),\n",
    "               gaussian_nb_clf()]\n",
    "\n",
    "def build_model(estimators, param_grid):\n",
    "    clf_name = estimators[0][0]\n",
    "    print('Start')\n",
    "    print('Classifier name: {}'.format(clf_name))\n",
    "    \n",
    "    pipeline_estimators = [('transformer', Transformer()),\n",
    "                           ('vectorizer', CountVectorizer(stop_words = 'english')),\n",
    "                           ('tfidf', TfidfTransformer()),\n",
    "                           ('mapper', Mapper()),\n",
    "                           estimators[0]]\n",
    "    \n",
    "    pipeline = Pipeline(pipeline_estimators)\n",
    "    clf = GridSearchCV(pipeline, param_grid = param_grid)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Best params:\\n', clf.best_params_, '\\n')\n",
    "    print('Classifier results:\\n', clf.cv_results_, '\\n')\n",
    "    y_pred = clf.predict(X_test)\n",
    "    cm = confusion_matrix(y_true = y_test, \n",
    "                          y_pred = y_pred, \n",
    "                          labels = ['1', '0'])\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "    report = classification_report(y_test, \n",
    "                                   y_pred, \n",
    "                                   target_names = ['0', '1'])\n",
    "    \n",
    "    # Save model\n",
    "    print('Classification report:\\n', report)\n",
    "    print('End')\n",
    "    return clf_name, clf\n",
    "\n",
    "# Unable to parallelize: Multiprocessing backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers = 4) as executor:\n",
    "#     futures = { executor.submit(build_model, estimators, param_grid): estimators[0][0]\n",
    "#                for (estimators, param_grid) in classifiers}\n",
    "#     for future in concurrent.futures.as_completed(future_to_url):\n",
    "#         out = futures[future]\n",
    "#         (clf_name, clf) = future.result()\n",
    "#     print('model saved as {}.pkl'.format(clf_name))\n",
    "#     joblib.dump(clf, '{}.pkl'.format(clf_name)) \n",
    "\n",
    "results = [build_model(estimators, param_grid) \n",
    "           for (estimators, param_grid) in classifiers]\n",
    "\n",
    "for (clf_name, clf) in results:\n",
    "    joblib.dump(clf, '{}.pkl'.format(clf_name)) \n",
    "    print('model saved as {}.pkl'.format(clf_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "clf = joblib.load('gaussian_nb.pkl')\n",
    "# clf = joblib.load('random_forest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0'], \n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['sexy babe', 'hello world']\n",
    "clf.predict(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
