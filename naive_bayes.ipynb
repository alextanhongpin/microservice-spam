{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load our sample data for both spam and ham. For this, we created a utility called loader which, given an input file path, reads the content from the file and append it to a python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loader(file_input):\n",
    "    data = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(file_input):\n",
    "        for file in filenames:\n",
    "            path = os.path.join(dirpath, file)\n",
    "            with open(path, encoding='latin-1') as f:\n",
    "                data.append(f.read())\n",
    "                f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_input = './data/enron1/ham'\n",
    "ham = loader(file_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_input = './data/enron1/spam'\n",
    "spam = loader(file_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "patt = re.compile(r'\\W')\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def process_words(data):\n",
    "    words = word_tokenize(data)\n",
    "    \n",
    "    # Lowercase\n",
    "#     words = [word.tolower() for word in words]\n",
    "\n",
    "    # Remove stop words\n",
    "#     words = [word for word in words if word not in stops]\n",
    "\n",
    "    # Remove special characters\n",
    "#     words = [word for word in words if not patt.search(word)]\n",
    "\n",
    "    # Remove digit\n",
    "#     words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "ham_data = [(process_words(words), 'pos') for words in ham]\n",
    "spam_data = [(process_words(words), 'neg') for words in spam]\n",
    "all_data = spam_data + ham_data\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier.train(all_data)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               forwarded = True              pos : neg    =    247.5 : 1.0\n",
      "                     hou = True              pos : neg    =    234.2 : 1.0\n",
      "                    2004 = True              neg : pos    =    198.2 : 1.0\n",
      "            prescription = True              neg : pos    =    160.7 : 1.0\n",
      "                     nom = True              pos : neg    =    154.3 : 1.0\n",
      "                    pain = True              neg : pos    =    128.1 : 1.0\n",
      "                    spam = True              neg : pos    =    108.5 : 1.0\n",
      "                     ect = True              pos : neg    =    103.9 : 1.0\n",
      "                     sex = True              neg : pos    =    103.6 : 1.0\n",
      "                featured = True              neg : pos    =     92.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'fake babe is amazing'\n",
    "text_data = process_words(text)\n",
    "clf.classify(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'december is amazing'\n",
    "text_data = process_words(text)\n",
    "clf.classify(text_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
