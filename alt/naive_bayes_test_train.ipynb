{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The first step is to load our sample data for both spam and ham. For this, we created a utility called loader which, given an input file path, reads the content from the file and append it to a python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loader(file_input):\n",
    "    data = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(file_input):\n",
    "        for file in filenames:\n",
    "            path = os.path.join(dirpath, file)\n",
    "            with open(path, encoding='latin-1') as f:\n",
    "                data.append(f.read())\n",
    "                f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_input = './data/enron1/ham'\n",
    "ham = loader(file_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_input = './data/enron1/spam'\n",
    "spam = loader(file_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "patt = re.compile(r'\\W')\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def process_words(data):\n",
    "    words = word_tokenize(data)\n",
    "    \n",
    "#     # Lowercase\n",
    "#     words = [word.lower() for word in words]\n",
    "\n",
    "#     # Remove stop words\n",
    "#     words = [word for word in words if word not in stops]\n",
    "\n",
    "#     # Remove special characters\n",
    "#     words = [word for word in words if not patt.search(word)]\n",
    "\n",
    "#     # Remove digit\n",
    "#     words = [word for word in words if not word.isdigit()]\n",
    "    \n",
    "#     # Strip\n",
    "#     words = [word.strip() for word in words]\n",
    "\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "ham_data = [(process_words(words), 'ham') for words in ham]\n",
    "spam_data = [(process_words(words), 'spam') for words in spam]\n",
    "all_data = spam_data + ham_data\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = np.array(all_data)\n",
    "X = all_data[:, 0]\n",
    "y = all_data[:, 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayesClassifier.train(zip(X_train, y_train))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               forwarded = True              ham : spam   =    164.7 : 1.0\n",
      "                     hou = True              ham : spam   =    157.0 : 1.0\n",
      "                  farmer = True              ham : spam   =    112.3 : 1.0\n",
      "            prescription = True             spam : ham    =    111.7 : 1.0\n",
      "                     nom = True              ham : spam   =    100.1 : 1.0\n",
      "                    pain = True             spam : ham    =     88.2 : 1.0\n",
      "                    2001 = True              ham : spam   =     86.1 : 1.0\n",
      "                creative = True             spam : ham    =     71.4 : 1.0\n",
      "                     ect = True              ham : spam   =     69.2 : 1.0\n",
      "                  health = True             spam : ham    =     64.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'fake babe is amazing'\n",
    "text_data = process_words(text)\n",
    "clf.classify(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'december is amazing'\n",
    "text_data = process_words(text)\n",
    "clf.classify(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = [clf.classify(X) for X in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual count: [('ham', 1191), ('spam', 516)]\n",
      "Predicted count: [('ham', 1119), ('spam', 588)]\n"
     ]
    }
   ],
   "source": [
    "actual_counts = Counter(y_test)\n",
    "predicted_counts = Counter(y_pred)\n",
    "\n",
    "print('Actual count:', actual_counts.most_common())\n",
    "print('Predicted count:', predicted_counts.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix is a tabular structure that helps visualize the performance of classifiers. Each column in the matrix represents classified instances based on predictions, and each row of the matrix represents classified instances based on the actual class labels.\n",
    "\n",
    "|            | p' (Predicted) | n' (Predicted) |\n",
    "|------------|----------------|----------------|\n",
    "| p (Actual) | True Positive  | False Negative |\n",
    "| n (Actual) | False Positive | True Negative  |\n",
    "\n",
    "- __True Positive (TP)__ indicates the number of correct hits or predictions for our positive class.\n",
    "- __False Negative (FN)__ indicates the number of instances we missed for that class by predicting it falsely as the negative class.\n",
    "- __False Positive (FP)__ is the number of instances we predicted wrongly as the positive class when it was actually not.\n",
    "- __True Negative (TN)__ is the number of instances we correctly predicted as the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 512,    4],\n",
       "       [  76, 1115]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_true = y_test, \n",
    "                      y_pred = y_pred, \n",
    "                      labels = ['spam', 'ham'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positive = 512\n",
      "true_negative = 1115\n",
      "false_positive = 76\n",
      "false_negative = 4\n"
     ]
    }
   ],
   "source": [
    "true_positive = cm[0][0]\n",
    "true_negative = cm[1][1]\n",
    "false_positive = cm[1][0]\n",
    "false_negative = cm[0][1]\n",
    "\n",
    "print('true_positive =', true_positive)\n",
    "print('true_negative =', true_negative)\n",
    "print('false_positive =', false_positive)\n",
    "print('false_negative =', false_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is defined as the overall accuracy or proportion of correct predictions of the model, which can be depicted by the formula:\n",
    "\n",
    "$Accuracy = \\cfrac{TP + TN }{TP + FP + FN + TN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 95.31%\n",
      "Manually computed accuracy = 95.31%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\n",
    "accuracy_manual = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "\n",
    "print('Accuracy = {:.2f}%'.format(accuracy * 100))\n",
    "print('Manually computed accuracy = {:.2f}%'.format(accuracy_manual * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Precision is defined as the number of predictions made that are actually correct or relevant out of all the predictions based on the positive class. This is also known as the _positive predictive value_ and can be depicted by the formula:\n",
    "\n",
    "$Precision = \\cfrac{TP}{TP + FP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 87.07%\n",
      "Manually computed precision = 87.07%\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_true = y_test, \n",
    "                            y_pred = y_pred,\n",
    "                            pos_label = 'spam')\n",
    "\n",
    "precision_manual = true_positive / (true_positive + false_positive)\n",
    "\n",
    "print('Precision = {:.2f}%'.format(precision * 100))\n",
    "print('Manually computed precision = {:.2f}%'.format(precision_manual * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall is defined as the number of instances of the positive class that were correctly predicted. This is also known as the _hit rate_, _coverage_, _sensitivity_ and can be depicted by the formula:\n",
    "\n",
    "$Recall = \\cfrac{TP}{TP + FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 99.22%\n",
      "Manually computed recall = 99.22%\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_true = y_test, \n",
    "                      y_pred = y_pred,\n",
    "                      pos_label = 'spam')\n",
    "\n",
    "recall_manual = true_positive / (true_positive + false_negative)\n",
    "\n",
    "print('Recall = {:.2f}%'.format(recall * 100))\n",
    "print('Manually computed recall = {:.2f}%'.format(recall_manual * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "\n",
    "F1 Score is another accuracy measure that is computed by taking the harmonic mean of the precision and recall and can be represented as follows:\n",
    "\n",
    "$F1 Score = \\cfrac{2 \\times Precision \\times Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 92.75%\n",
      "Manually computed F1-Score = 92.75%\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_true = y_test, \n",
    "              y_pred = y_pred,\n",
    "              pos_label = 'spam')\n",
    "f1_manual = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "print('F1-Score = {:.2f}%'.format(f1 * 100))\n",
    "print('Manually computed F1-Score = {:.2f}%'.format(f1_manual * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
